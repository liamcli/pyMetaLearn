{
 "metadata": {
  "name": "",
  "signature": "sha256:1d1c86f346d01c0ff59006b197cba5febb2d2129a293e71f064a6bd4c2f707cc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from argparse import ArgumentParser\n",
      "from collections import OrderedDict\n",
      "import cPickle\n",
      "import glob\n",
      "import itertools\n",
      "import numpy as np\n",
      "import os\n",
      "import re\n",
      "import scipy.stats\n",
      "import sys\n",
      "\n",
      "%matplotlib inline\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib import rc\n",
      "rc('text', usetex=True)\n",
      "\n",
      "import HPOlib.Plotting.plot_util as plot_util\n",
      "import HPOlib.Plotting.generateTexTable as generate_tex_table\n",
      "import HPOlib.Plotting.plotTraceWithStd_perEval as plotTraceWithStd_perEval\n",
      "import HPOlib.Plotting.statistics as statistics\n",
      "\n",
      "import pyMetaLearn.openml.manage_openml_data\n",
      "import pyMetaLearn.metafeatures.metafeatures as mf_module\n",
      "from pyMetaLearn.openml.openml_task import OpenMLTask\n",
      "import pyMetaLearn.workflows.plot_utils as meta_plot_util"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ground_truth_dir = \"/home/feurerm/thesis/experiments/2014_05_16_MiniAutoSklearn_meta/\"\n",
      "experiments_directory = \"/home/feurerm/thesis/experiments/2014_06_01_AutoSklearn_metalearning/\"\n",
      "plot_dir = os.path.join(experiments_directory, \"plots\")\n",
      "pyMetaLearn.openml.manage_openml_data.set_local_directory(\n",
      "    \"/home/feurerm/thesis/datasets/openml/\")\n",
      "local_directory = pyMetaLearn.openml.manage_openml_data.get_local_directory()\n",
      "openml_dataset_dir = os.path.join(local_directory, \"datasets\")\n",
      "    \n",
      "num_folds = 3\n",
      "try:\n",
      "    os.mkdir(plot_dir)\n",
      "except:\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset_list = glob.glob(os.path.join(openml_dataset_dir, \"did*.xml\"))\n",
      "datasets = []\n",
      "dataset_names = []\n",
      "for dataset_filename in dataset_list:\n",
      "    did = os.path.split(dataset_filename)[1]\n",
      "    did = int(did.replace(\"did\", \"\").replace(\".xml\", \"\"))\n",
      "\n",
      "    task_file = os.path.join(pyMetaLearn\n",
      "                .openml.manage_openml_data.get_local_directory(),\n",
      "                \"custom_tasks\", \"did_%d.pkl\" % did)\n",
      "    if not os.path.exists(task_file):\n",
      "        # print \"Skipping dataset %s\" % dataset_filename\n",
      "        continue\n",
      "        \n",
      "    with open(task_file) as fh:\n",
      "        task_dict = cPickle.load(fh)\n",
      "        \n",
      "    dataset = pyMetaLearn.openml.manage_openml_data.get_local_dataset(task_dict['data_set_id'])\n",
      "    dataset_names.append(dataset._name)\n",
      "    \n",
      "    datasets.append(did)\n",
      "datasets = datasets\n",
      "datasets.sort()\n",
      "\n",
      "print datasets, len(datasets)\n",
      "print dataset_names"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 20, 21, 22, 23, 24, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 48, 49, 50, 51, 53, 54, 55, 56, 58, 59, 60, 61, 62, 171, 181, 182, 183, 186, 188] 57\n",
        "[u'vehicle', u'anneal.ORIG', u'segment', u'pendigits', u'mushroom', u'labor', u'abalone', u'diabetes', u'car', u'autos', u'mfeat-fourier', u'primary-tumor', u'waveform-5000', u'credit-a', u'letter', u'postoperative-patient-data', u'vowel', u'eucalyptus', u'cylinder-bands', u'ionosphere', u'mfeat-karhunen', u'balance-scale', u'mfeat-morphological', u'liver-disorders', u'dermatology', u'vote', u'breast-cancer', u'credit-g', u'soybean', u'arrhythmia', u'nursery', u'hepatitis', u'cmc', u'heart-c', u'glass', u'braziltourism', u'iris', u'heart-statlog', u'page-blocks', u'breast-w', u'heart-h', u'yeast', u'tae', u'haberman', u'mfeat-pixel', u'optdigits', u'mfeat-factors', u'satimage', u'lymph', u'audiology', u'ecoli', u'tic-tac-toe', u'spambase', u'zoo', u'sonar', u'mfeat-zernike', u'kr-vs-kp']\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Here could be code get the metafeatures of a single dataset"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Assemble the list of optimizers\n",
      "bootstraps =  (2,) # 5) #, 10)\n",
      "distance = (\"l1\", \"l2\")# \"learned\")\n",
      "metafeature_subset = mf_module.subsets\n",
      "\n",
      "optimizers = OrderedDict([\n",
      "    (\"SMAC\", \"%s/smac_2_06_01-dev_*/smac_2_06_01-dev.pkl\"),\n",
      "    (\"random\", \"%s/random_hyperopt_august2013_mod*/random_hyperopt_august2013_mod.pkl\"),\n",
      "    (\"TPE\", \"%s/hyperopt_august2013_mod_*/hyperopt_august2013_mod.pkl\"),\n",
      "    #(\"SMAC(WS5,l1,all)\", \"%s/bootstrapped5_l1_allsmac_warmstart_*/*smac_warmstart.pkl\"),\n",
      "    #(\"SMAC(WS5,l1,pfahringer)\", \"%s/bootstrapped5_l1_pfahringer_2000_experiment1smac_warmstart_*/*smac_warmstart.pkl\"),\n",
      "    #(\"SMAC(WS5,l1,yogatama)\", \"%s/bootstrapped5_l1_yogotama_2014smac_warmstart_*/*smac_warmstart.pkl\"),\n",
      "    #(\"SMAC(WS5,learned,all)\", \"%s/bootstrapped5_learned_distance_allsmac_warmstart_*/*smac_warmstart.pkl\"),\n",
      "    #(\"SMAC(WS5,learned,pfahringer)\", \"%s/bootstrapped5_learned_distance_pfahringer_2000_experiment1smac_warmstart_*/*smac_warmstart.pkl\"),\n",
      "    #(\"MI-SMAC(10,$L_1$,all)\", \"%s/bootstrapped10_l1_allsmac_warmstart_*/*smac_warmstart.pkl\"),\n",
      "    #(\"MI-SMAC(10,$L_1$,landmarking)\", \"%s/bootstrapped10_l1_pfahringer_2000_experiment1smac_warmstart_*/*smac_warmstart.pkl\"),\n",
      "    #(\"MI-SMAC(15,$L_1$,all)\", \"%s/bootstrapped15_l1_allsmac_warmstart_*/*smac_warmstart.pkl\"),\n",
      "    #(\"MI-SMAC(15,$L_1$,landmarking)\", \"%s/bootstrapped15_l1_pfahringer_2000_experiment1smac_warmstart_*/*smac_warmstart.pkl\"),\n",
      "    (\"MI-SMAC(20,$L_1$,all)\", \"%s/bootstrapped20_l1_allsmac_warmstart_*/*smac_warmstart.pkl\"),\n",
      "    (\"MI-SMAC(20,$L_1$,landmarking)\", \"%s/bootstrapped20_l1_pfahringer_2000_experiment1smac_warmstart_*/*smac_warmstart.pkl\"),\n",
      "    (\"MI-SMAC(25,$L_1$,all)\", \"%s/bootstrapped25_l1_allsmac_warmstart_*/*smac_warmstart.pkl\"),\n",
      "    (\"MI-SMAC(25,$L_1$,landmarking)\", \"%s/bootstrapped25_l1_pfahringer_2000_experiment1smac_warmstart_*/*smac_warmstart.pkl\")])\n",
      "    #(\"MI-SMAC(30,$L_1$,all)\", \"%s/bootstrapped30_l1_allsmac_warmstart_*/*smac_warmstart.pkl\"),\n",
      "    #(\"MI-SMAC(30,$L_1$,landmarking)\", \"%s/bootstrapped30_l1_pfahringer_2000_experiment1smac_warmstart_*/*smac_warmstart.pkl\")])\n",
      "    #(\"SMAC(WS10,l1,yogatama)\", \"%s/bootstrapped5_l1_yogotama_2014smac_warmstart_*/*smac_warmstart.pkl\"),\n",
      "    #(\"SMAC(WS10,learned,all)\", \"%s/bootstrapped10_learned_distance_allsmac_warmstart_*/*smac_warmstart.pkl\"),\n",
      "    #(\"SMAC(WS10,learned,pfahringer)\", \"%s/bootstrapped10_learned_distance_pfahringer_2000_experiment1smac_warmstart_*/*smac_warmstart.pkl\")])\n",
      "    #(\"SMAC/WS2\", \"%s/bootstrapped2_learnedspearmint_gitfork_mod_*/*spearmint_gitfork_mod.pkl\"), \n",
      "    #(\"SMAC/WS5\", \"%s/bootstrapped5_learnedspearmint_gitfork_mod_*/*spearmint_gitfork_mod.pkl\")])\n",
      "\n",
      "#for dist, subset in itertools.product(distance, metafeature_subset, repeat=1):\n",
      "#    optimizers[\"ML_%s_%s\" % (dist, subset)] = \\\n",
      "#        \"%s\" + \"/%s_%smetalearn_optimizer*/*metalearn_optimizer.pkl\" % (dist, subset)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_folds = 1\n",
      "# Plot the error trace over all experiments\n",
      "include_in_plots = {\"\": optimizers,\n",
      "                    \"smbo\": [\"SMAC\", \"TPE\", \"random\"],\n",
      "                    \"bs\": [\"SMAC\", \"MI-SMAC(10,$L_1$,all)\", \"MI-SMAC(10,$L_1$,landmarking)\"],\n",
      "                    \"smbo+best\": [\"SMAC\", \"TPE\", \"random\", \"MI-SMAC(10,$L_1$,landmarking)\"]}\n",
      "\n",
      "for iip in include_in_plots:\n",
      "    try:\n",
      "        print os.path.join(\"%s\" % plot_dir, \"%s\" % iip)\n",
      "        os.makedirs(os.path.join(\"%s\" % plot_dir, \"%s\" % iip))\n",
      "    except Exception as e:\n",
      "        print e\n",
      "        \n",
      "    gigantic_pickle_list = [[] for optimizer in include_in_plots[iip]]\n",
      "    for idx, dataset in enumerate(datasets):\n",
      "        dataset_rankings = np.zeros((50, len(include_in_plots[iip])), dtype=np.float64)\n",
      "        for fold in range(num_folds):\n",
      "            dataset_dir = \"did_%d_fold%d\" % (dataset, fold)\n",
      "            exp_dir = os.path.join(experiments_directory, dataset_dir)\n",
      "            argument_list = []\n",
      "            for optimizer in include_in_plots[iip]:\n",
      "                pkls = glob.glob(optimizers[optimizer] % exp_dir)\n",
      "                argument_list.append(optimizer)\n",
      "                argument_list.extend(pkls)\n",
      "            try:\n",
      "                pkl_list_main, name_list_main = plot_util.get_pkl_and_name_list(argument_list)\n",
      "            except ValueError as e:\n",
      "                print \"Value Error in dataset directory %s\" % dataset_dir\n",
      "                raise ValueError\n",
      "            for i, optimizer in enumerate(include_in_plots[iip]):\n",
      "                gigantic_pickle_list[i].extend(pkl_list_main[i])\n",
      "        \n",
      "    for file_suffix in [\"png\", \"pdf\"]:\n",
      "        plotTraceWithStd_perEval.main(gigantic_pickle_list, name_list_main, True,\n",
      "                                      save=\"%s/%s/all_datasets_error.%s\" % (plot_dir, iip, file_suffix),\n",
      "                                      markers=False, linewidth=1, linestyle=True, y_max=0.4)\n",
      "        plotTraceWithStd_perEval.main(gigantic_pickle_list, name_list_main, True,\n",
      "                                      save=\"%s/%s/all_datasets_log_error.%s\" % (plot_dir, iip, file_suffix),\n",
      "                                      log=True, markers=False, linewidth=1, linestyle=True, y_max=-0.5, y_min=-0.9)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the per-dataset error traces\n",
      "best_parameters = dict()\n",
      "\n",
      "include_in_plots = {\"\": optimizers,\n",
      "                    \"smbo\": [\"SMAC\", \"TPE\", \"random\"],\n",
      "                    \"bs\": [\"SMAC\", \"MI-SMAC(10,$L_1$,all)\", \"MI-SMAC(10,$L_1$,landmarking)\"],\n",
      "                    \"smbo+best\": [\"SMAC\", \"TPE\", \"random\", \"MI-SMAC(10,$L_1$,landmarking)\"]}\n",
      "\n",
      "for iip in include_in_plots:\n",
      "    try:\n",
      "        print os.path.join(\"%s\" % plot_dir, \"%s\" % iip)\n",
      "        os.makedirs(os.path.join(\"%s\" % plot_dir, \"%s\" % iip))\n",
      "    except Exception as e:\n",
      "        print e\n",
      "    \n",
      "    for idx, dataset in enumerate(datasets):\n",
      "        for fold in range(num_folds):\n",
      "            dataset_dir = \"%s/did_%d_fold%d\" % (experiments_directory, dataset, fold)\n",
      "            plot_suffix = \"did_%d_fold%d\" % (dataset, fold)\n",
      "            if not os.path.isdir(dataset_dir) or \"did\" not in dataset_dir:\n",
      "                continue\n",
      "\n",
      "            argument_list = []\n",
      "            for optimizer in optimizers:\n",
      "                if optimizer in include_in_plots[iip]:\n",
      "                    pkls = glob.glob(optimizers[optimizer] % dataset_dir)\n",
      "                    argument_list.append(optimizer)\n",
      "                    argument_list.extend(pkls)\n",
      "\n",
      "            pkl_list_main, name_list_main = plot_util.get_pkl_and_name_list(argument_list)\n",
      "\n",
      "            ground_truth = meta_plot_util.find_ground_truth(os.path.join(ground_truth_dir,\n",
      "                                                            \"did_%d_fold%s\" % (dataset, fold),\n",
      "                                                            \"sequential_optimization_*_\",\n",
      "                                                            \"sequential.pkl\"))\n",
      "            if not ground_truth:\n",
      "                continue\n",
      "\n",
      "            ########################################################################\n",
      "            # Plot error traces for one dataset\n",
      "            optimum, trial_index = plot_util.get_best_value_and_index(ground_truth)\n",
      "            best_parameters[dataset] = ground_truth['trials'][trial_index]['params']\n",
      "\n",
      "            for file_suffix in [\"pdf\"]:#, \"png\"]:\n",
      "                #plotTraceWithStd_perEval.main(pkl_list_main, name_list_main, True, \n",
      "                #                          save=\"%s/%s/error_trace_%s.%s\" % (plot_dir, iip, plot_suffix, file_suffix),\n",
      "                #                          markers=False, linewidth=1, linestyle=True);\n",
      "                \n",
      "                #plotTraceWithStd_perEval.main(pkl_list_main, name_list_main, True, log=True, \n",
      "                #                          save=\"%s/%s/error_trace_log_%s.%s\" % (plot_dir, iip, plot_suffix, file_suffix),\n",
      "                #                          markers=False, linewidth=1, linestyle=True);\n",
      "\n",
      "                plotTraceWithStd_perEval.main(pkl_list_main, name_list_main, True, optimum=optimum,\n",
      "                                          save=\"%s/%s/optimum_error_trace_%s.%s\" % (plot_dir, iip, plot_suffix, file_suffix),\n",
      "                                          markers=False, linewidth=1, linestyle=True);\n",
      "                \n",
      "                #plotTraceWithStd_perEval.main(pkl_list_main, name_list_main, True, optimum=optimum, log=True,\n",
      "                #                          save=\"%s/%s/optimum_error_trace_log_%s.%s\" % (plot_dir, iip, plot_suffix, file_suffix),\n",
      "                #                          markers=False, linewidth=1, linestyle=True);\n",
      "                \n",
      "        \n",
      "# Plot this to a file or something...\n",
      "# for idx, dataset in enumerate(datasets):\n",
      "#     print dataset, dataset_names[idx], best_parameters[dataset]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Do the statistical stuff\n",
      "include_in_plots = {\"\": optimizers,\n",
      "                    \"smbo\": [\"SMAC\", \"TPE\", \"random\"],\n",
      "                    \"bs\": [\"SMAC\", \"MI-SMAC(10,$L_1$,all)\", \"MI-SMAC(10,$L_1$,landmarking)\"],\n",
      "                    \"smbo+best\": [\"SMAC\", \"TPE\", \"random\", \"MI-SMAC(10,$L_1$,landmarking)\"]}\n",
      "reload(meta_plot_util)\n",
      "for iip in include_in_plots:\n",
      "    try:\n",
      "        print os.path.join(\"%s\" % plot_dir, \"%s\" % iip)\n",
      "        os.makedirs(os.path.join(\"%s\" % plot_dir, \"%s\" % iip))\n",
      "    except Exception as e:\n",
      "        print e\n",
      "    \n",
      "    trial_list_per_dataset = []\n",
      "    trial_list_per_dataset2 = []\n",
      "    for fold in range(num_folds):\n",
      "        for idx, dataset in enumerate(datasets):\n",
      "            dataset_dir = \"%s/did_%d_fold%d\" % (experiments_directory, dataset, fold)\n",
      "            if not os.path.isdir(dataset_dir) or \"did\" not in dataset_dir:\n",
      "                continue\n",
      "\n",
      "            argument_list = []\n",
      "            for optimizer in optimizers:\n",
      "                \n",
      "                if optimizer not in [\"MI-SMAC(20,$L_1$,landmarking)\", \"MI-SMAC(25,$L_1$,landmarking)\", 'SMAC']:\n",
      "                    continue\n",
      "                if optimizer in include_in_plots[iip]:\n",
      "                    pkls = glob.glob(optimizers[optimizer] % dataset_dir)\n",
      "                    argument_list.append(optimizer)\n",
      "                    argument_list.extend(pkls)\n",
      "                        \n",
      "            pkl_list_main, name_list_main = plot_util.get_pkl_and_name_list(argument_list)\n",
      "            trial_list_per_dataset.append(pkl_list_main)\n",
      "\n",
      "    for file_suffix in [\"png\", \"pdf\"]:\n",
      "        meta_plot_util.plot_summed_wins_of_optimizers(\n",
      "            trial_list_per_dataset, name_list_main, legend_ncols=2,\n",
      "            save=(\"%s/%s/percentage_of_wins_\" % (plot_dir, iip.replace(\"/\", \"_\")) + \"%s.\" + file_suffix))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/feurerm/thesis/experiments/2014_06_01_AutoSklearn_metalearning/plots/\n",
        "[Errno 17] File exists: '/home/feurerm/thesis/experiments/2014_06_01_AutoSklearn_metalearning/plots/'\n",
        "/home/feurerm/thesis/experiments/2014_06_01_AutoSklearn_metalearning/plots/smbo"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[Errno 17] File exists: '/home/feurerm/thesis/experiments/2014_06_01_AutoSklearn_metalearning/plots/smbo'\n",
        "/home/feurerm/thesis/experiments/2014_06_01_AutoSklearn_metalearning/plots/smbo+best"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[Errno 17] File exists: '/home/feurerm/thesis/experiments/2014_06_01_AutoSklearn_metalearning/plots/smbo+best'\n",
        "/home/feurerm/thesis/experiments/2014_06_01_AutoSklearn_metalearning/plots/bs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[Errno 17] File exists: '/home/feurerm/thesis/experiments/2014_06_01_AutoSklearn_metalearning/plots/bs'\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/home/feurerm/thesis/virtualenvs/development/local/lib/python2.7/site-packages/matplotlib/axes.py:2760: UserWarning: Attempting to set identical bottom==top results\n",
        "in singular transformations; automatically expanding.\n",
        "bottom=0, top=0.0\n",
        "  + 'bottom=%s, top=%s') % (bottom, top))\n",
        "/home/feurerm/thesis/virtualenvs/development/local/lib/python2.7/site-packages/matplotlib/axes.py:2760: UserWarning: Attempting to set identical bottom==top results\n",
        "in singular transformations; automatically expanding.\n",
        "bottom=-0.0, top=0\n",
        "  + 'bottom=%s, top=%s') % (bottom, top))\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the rankings\n",
      "num_folds = 1\n",
      "include_in_plots = {\"\": optimizers,\n",
      "                    \"smbo\": [\"SMAC\", \"TPE\", \"random\"],\n",
      "                    \"bs\": [\"SMAC\", \"MI-SMAC(10,$L_1$,all)\", \"MI-SMAC(10,$L_1$,landmarking)\"],\n",
      "                    \"smbo+best\": [\"SMAC\", \"TPE\", \"random\", \"MI-SMAC(10,$L_1$,landmarking)\"]}\n",
      "rankings = {}\n",
      "\n",
      "for iip in include_in_plots:\n",
      "    num_datasets = 0\n",
      "    try:\n",
      "        print os.path.join(\"%s\" % plot_dir, \"%s\" % iip)\n",
      "        os.makedirs(os.path.join(\"%s\" % plot_dir, \"%s\" % iip))\n",
      "    except Exception as e:\n",
      "        print e\n",
      "\n",
      "    rankings[iip] = np.zeros((50, len(include_in_plots[iip])), dtype=np.float64)\n",
      "    for idx, dataset in enumerate(datasets):\n",
      "        dataset_rankings = np.zeros((50, len(include_in_plots)), dtype=np.float64)\n",
      "        for fold in range(num_folds):\n",
      "            dataset_dir = \"%s/did_%d_fold%d\" % (experiments_directory, dataset, fold)\n",
      "            plot_suffix = \"did_%d_fold%d\" % (dataset, fold)\n",
      "            if not os.path.isdir(dataset_dir) or \"did\" not in dataset_dir:\n",
      "                continue\n",
      "\n",
      "            print dataset_dir\n",
      "            num_datasets += 1\n",
      "\n",
      "            argument_list = []\n",
      "            for optimizer in include_in_plots[iip]:\n",
      "                pkls = glob.glob(optimizers[optimizer] % dataset_dir)\n",
      "                argument_list.append(optimizer)\n",
      "                argument_list.extend(pkls)\n",
      "\n",
      "            pkl_list_main, name_list_main = plot_util.get_pkl_and_name_list(argument_list)\n",
      "            ranking = meta_plot_util.plot_rankings(pkl_list_main, name_list_main, save=\"%s/%s/ranks_%s.pdf\" % (plot_dir, iip,  plot_suffix), figsize=(16,6))\n",
      "            #ranking = meta_plot_util.plot_rankings(pkl_list_main, name_list_main, save=\"%s/%s/ranks_%s.png\" % (plot_dir, iip,  plot_suffix), figsize=(16,6))\n",
      "            rankings[iip] += ranking\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/feurerm/thesis/experiments/2014_06_01_AutoSklearn_metalearning/plots/\n",
        "[Errno 17] File exists: '/home/feurerm/thesis/experiments/2014_06_01_AutoSklearn_metalearning/plots/'\n",
        "/home/feurerm/thesis/experiments/2014_06_01_AutoSklearn_metalearning//did_2_fold0\n",
        "0.0001"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-7-0b9b56c5f094>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mpkl_list_main\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_list_main\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplot_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_pkl_and_name_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margument_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mranking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_plot_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_rankings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpkl_list_main\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_list_main\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"%s/%s/ranks_%s.pdf\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mplot_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miip\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mplot_suffix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[1;31m#ranking = meta_plot_util.plot_rankings(pkl_list_main, name_list_main, save=\"%s/%s/ranks_%s.png\" % (plot_dir, iip,  plot_suffix), figsize=(16,6))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mrankings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miip\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mranking\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/feurerm/thesis/Software/pyMetaLearn/pyMetaLearn/workflows/plot_utils.pyc\u001b[0m in \u001b[0;36mplot_rankings\u001b[1;34m(trial_list, name_list, optimum, title, log, save, y_min, y_max, cut, figsize, legend_ncols)\u001b[0m\n\u001b[0;32m     66\u001b[0m             ranks = scipy.stats.rankdata(\n\u001b[0;32m     67\u001b[0m                 [np.round(plot_util.get_best(pickles[optimizers[idx]][number], i), 5)\n\u001b[1;32m---> 68\u001b[1;33m                  for idx, number in enumerate(product)])\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mnum_products\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/feurerm/HPOlib/Software/HPOlib/HPOlib/Plotting/plot_util.pyc\u001b[0m in \u001b[0;36mget_best\u001b[1;34m(trials, cut)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_best\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;31m# returns the best value found in this experiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m     \u001b[0mtraj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_trajectory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m     \u001b[0mbest_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mcut\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/feurerm/HPOlib/Software/HPOlib/HPOlib/Plotting/plot_util.pyc\u001b[0m in \u001b[0;36mextract_trajectory\u001b[1;34m(trials, cut)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mcurrentbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"result\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'trials'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "################################################################################\n",
      "# draw a ranking graph averaged over all datasets\n",
      "num_datasets=57\n",
      "for iip in include_in_plots:\n",
      "    plt.figure(dpi=600, figsize=(9, 6))\n",
      "    ax = plt.subplot(111)\n",
      "    colors = plot_util.get_plot_colors()\n",
      "    ranks = rankings[iip] / float(num_datasets)\n",
      "    for i, optimizer in enumerate(include_in_plots[iip]):\n",
      "            ax.plot(range(1, 51), ranks[:, i], color=colors.next(),\n",
      "            linewidth=3, label=optimizer.replace(\"\\\\\", \"\"))\n",
      "\n",
      "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
      "              fancybox=True, shadow=True, ncol=3, labelspacing=0.25, fontsize=12)\n",
      "    box = ax.get_position()\n",
      "\n",
      "    ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
      "                 box.width, box.height * 0.9])\n",
      "    for plot_suffix in ['png', 'pdf']:\n",
      "        plt.savefig(\"%s/%s/all_datasets.%s\" % (plot_dir, iip, plot_suffix))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####################################################################\n",
      "# For a dataset, write out which hyperparameters are selected by all\n",
      "# metafeatures and the pfahringer metafeatures\n",
      "plot_dir = \"/home/feurerm/tmp/simple_metalearning_plots/\"\n",
      "num_folds = 1\n",
      "\n",
      "print \"WARNING: the ratio is overestimated as the metafeature computation time is for the whole dataset\" \\\n",
      "    \" and not only for the training set which contains only 2/3 of the data.\"\n",
      "for idx, did in enumerate(datasets):\n",
      "    for fold in range(num_folds):\n",
      "        dataset_dir = \"%s/did_%d_fold%d\" % (experiments_directory, did, fold)\n",
      "        plot_suffix = \"did_%d_fold%d\" % (did, fold)\n",
      "        if not os.path.isdir(dataset_dir) or \"did\" not in dataset_dir:\n",
      "            continue\n",
      "            \n",
      "        ground_truth = meta_plot_util.find_ground_truth(os.path.join(ground_truth_dir,\n",
      "                                                        \"did_%d_fold%s\" % (did, fold),\n",
      "                                                        \"sequential_optimization_*_\",\n",
      "                                                        \"sequential.pkl\"))\n",
      "        if not ground_truth:\n",
      "            continue\n",
      "        optimum, trial_index = plot_util.get_best_value_and_index(ground_truth)\n",
      "        best_parameters = ground_truth['trials'][trial_index]['params']\n",
      "        \n",
      "        dataset = pyMetaLearn.openml.manage_openml_data.get_local_dataset(did)\n",
      "        # Here should be a splitting file, as it isn't there, the costs are overestimated\n",
      "        metafeatures, calculation_time = dataset.get_metafeatures(return_times=True)\n",
      "        metafeatures_calculation_time = sum(calculation_time.values())\n",
      "        \n",
      "        # Somehow read the calculation time of the metafeatures\n",
      "        times = []\n",
      "        for trial in ground_truth['trials']:\n",
      "            #times.extend(trial['instance_durations'])\n",
      "            times.append(trial['duration'])\n",
      "        print did, np.nansum(times), np.nanmean(times), np.nanstd(times), np.nanmin(times), \\\n",
      "            np.nanmax(times), metafeatures_calculation_time, metafeatures_calculation_time / np.nanmean(times)\n",
      "        \"\"\"\n",
      "        metafeatures_all_file = glob.glob(\"%s/l1_allmetalearn_optimizer_*/l1_allmetalearn_optimizer.pkl\" % dataset_dir)\n",
      "        assert len(metafeatures_all_file) == 1\n",
      "        with open(metafeatures_all_file[0]) as fh:\n",
      "            metafeatures_all_trials = cPickle.load(fh)\n",
      "            \n",
      "        metafeatures_pfahringer_file = glob.glob(\"%s/l1_pfahringer_2000_experiment1\"\n",
      "            \"metalearn_optimizer_*/l1_pfahringer_2000_experiment1metalearn_optimizer.pkl\" % dataset_dir)\n",
      "        assert len(metafeatures_pfahringer_file) == 1\n",
      "        with open(metafeatures_pfahringer_file[0]) as fh:\n",
      "            metafeatures_pfahringer_trials = cPickle.load(fh)\n",
      "                        \n",
      "        with open(os.path.join(plot_dir, plot_suffix + \".txt\"), \"w\") as fh:\n",
      "            fh.write(str(best_parameters))\n",
      "            fh.write(\"\\n\")\n",
      "            for trial0, trial1 in itertools.izip(metafeatures_all_trials['trials'], metafeatures_pfahringer_trials['trials']):\n",
      "                fh.write(', '.join([str(param) + \": \" + trial0['params'][param] for param in trial0['params']]))\n",
      "                fh.write(\"   \")\n",
      "                fh.write(', '.join([str(param) + \": \" + trial1['params'][param] for param in trial1['params']]))\n",
      "                fh.write(\"\\n\")\n",
      "        \"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.std([1.65, 1.75])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "0.050000000000000044"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}